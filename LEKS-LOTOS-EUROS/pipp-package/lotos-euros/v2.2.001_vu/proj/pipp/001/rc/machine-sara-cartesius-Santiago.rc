!#######################################################################
!
! TNO / HPC3 computing server
!
!#######################################################################


!----------------------------------------------------------------------
! user names
!----------------------------------------------------------------------

! used for output attributes ...

! name of institution:
user.institution              :  VU

! extract user long name from environment variable:
user.longname                 :  ${USER}


!----------------------------------------------------------------------
! environment
!----------------------------------------------------------------------

! Toolchain modules (2020):
my.toolchain.name             :  intel
my.toolchain.version          :  2018b
! module commands, first system, then locally installed:
*.modules                     :  purge ; \
                                   load 2019 ; \
                                   load eb ; \
                                   load ${my.toolchain.name}/${my.toolchain.version} ; \
                                   load netCDF-Fortran/4.4.4-${my.toolchain.name}-${my.toolchain.version} ; \
                                   load UDUNITS/2.2.26-${my.toolchain.name}-${my.toolchain.version} ; \
                                   load Anaconda3/2018.12 ; \
                                   use /home/slr/admin/modulefiles ; \ 
                                   load tm5 ; \
                                   load makedepf90
!! modules loaded in job scripts:
!*.modules                     :  purge ; \
!                                 load slurm/18.08.8 ; \
!                                 load anaconda3/2019.03 ; \
!                                 load curl/default ; \
!                                 load openssl/default ; \
!                                 load gcc-suite/8.2.0 ; \
!                                 load openmpi/4.0.5 ; \
!                                 load szip/2.1.1 ; \
!                                 load hdf5/1.12.0 ; \
!                                 load netcdf-c/4.7.4 ; \
!                                 load netcdf-fortran/4.5.3 ; \
!                                 load udunits/2.2.26 ; \
!                                 load makedepf90 ; \
!                                 load nco/4.8.1 ; \
!                                 load lapack/3.6.1 ; \
!                                 load spblas/1.02.917 ; \
!                                 load lotos-euros


!----------------------------------------------------------------------
! tasks and threads
!----------------------------------------------------------------------

! number of MPI tasks in run step:
#if "${par.mpi}" in ["True"] :
my.run.ntask                      :  ${par.ntask}
#elif "${par.mpi}" in ["False"] :
my.run.ntask                      :  1
#else
#error Could not set number of MPI tasks for par.mpi "${par.mpi}"
#endif

! number of OpenMP threads in run step:
#if "${par.openmp}" in ["True"] :
my.run.nthread                    :  ${par.nthread}
#elif "${par.openmp}" in ["False"] :
my.run.nthread                    :  1
#else
#error Could not set number of OpenMP threads for par.openmp "${par.openmp}"
#endif


!----------------------------------------------------------------------
! job scripts
!----------------------------------------------------------------------

! default class with the job script creator:
*.script.class           :  utopya.UtopyaJobScriptBatchSlurm

! Define batch job option format for class 'JobScriptBatchSlurm':
!   #SBATCH --flag=value
slurm_format.comment       :  #
slurm_format.prefix        :  SBATCH
slurm_format.arg           :  '--'
slurm_format.assign        :  '='
slurm_format.template      :  %(key)
slurm_format.envtemplate   :  %(env:key)

! job format for this application:
*.batch.slurm.format            :  slurm_format

! which keywords:
*.batch.slurm.options           :  name partition nodes ntasks nproc memory time workdir output error

! job name, take from job environment:
*.batch.slurm.option.name            :  job-name %(env:name)

! partitions:
! - normal    # always full node
! - short     # always full node, max 1 hour
! - staging   # archiving etc
!*.batch.slurm.option.partition       :  partition short
*.batch.slurm.option.partition       :  partition staging

! single node, single task:
*.batch.slurm.option.nodes           :  nodes  1
*.batch.slurm.option.ntasks          :  ntasks 1
! single core:
*.batch.slurm.option.nproc           :  cpus-per-task 1

! max memory:
*.batch.slurm.option.memory          :  mem 1Gb

! max run time MM:SS
*.batch.slurm.option.time            :  time 5:00

! work directory, use current workdir from job environment:
*.batch.slurm.option.workdir         :  chdir %(env:cwd)

! log files, use the job name as base:
*.batch.slurm.option.output          :  output %(name).out
*.batch.slurm.option.error           :  error  %(name).err

! specific:
#if (${my.run.ntask} > 1) or (${my.run.nthread} > 1)
jobtree.le.run.batch.slurm.option.partition  :  partition short
jobtree.le.run.batch.slurm.option.ntasks     :  ntasks ${my.run.ntask}
jobtree.le.run.batch.slurm.option.threads    :  cpus-per-task ${my.run.nthread}
#endif
jobtree.le.run.batch.slurm.option.memory     :  mem ${my.run.memory}Gb
jobtree.le.run.batch.slurm.option.time       :  time 1:00:00



!----------------------------------------------------------------------
! runner
!----------------------------------------------------------------------

! run model:
#if "${par.mpi}" in ["True"]

! SLURM version of "mpirun" ;
! number of tasks etc taken from batch job environment;
! create stdout/stderr files files including task number:
le.run.command     :  srun \
                        --kill-on-bad-exit=1 \
                        --output=le.run.out.%t \
                        --error=le.run.err.%t \
                        ./lotos-euros.x ${my.le.run.rcfile}

#else

! run model, pass evaluated rcfile as argument:
le.run.command     :  ./lotos-euros.x ${my.le.run.rcfile}

#endif


!----------------------------------------------------------------------
! compiler settings
!----------------------------------------------------------------------

! intel suite:
#include proj/pipp/${my.le.patch}/rc/compiler-intel-18.0.3.rc

! MPI wrappers:
configure.compiler.fc.mpi                   :  mpiifort
configure.compiler.fc.mpi.openmp            :  mpiifort


!----------------------------------------------------------------------
! libraries
!----------------------------------------------------------------------

! (optional) macro to enable udunits library:
!my.udunits.define             :  
!my.udunits.define             :  with_udunits1
my.udunits.define             :  with_udunits2

! define libraries:
my.spblas.define              :  
!my.spblas.define              :  with_spblas

! ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

! ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
! dummy values, will be replaced by actual environment values ...
EBROOTNETCDFMINFORTRAN      : /opt
HDF_HOME                    : /opt
EBROOTUDUNITS               : /opt
EBROOTIMKL                  : /opt


! NetCDF4 library:
configure.lib.netcdf.fflags       : -I${EBROOTNETCDFMINFORTRAN}/include
configure.lib.netcdf.ldflags      : -lnetcdff -lnetcdf

! UDUnits library:
configure.lib.udunits2.fflags     : -I${EBROOTUDUNITS}/include
configure.lib.udunits2.ldflags    : -ludunits2

!! sequential version:
!configure.lib.mkl.fflags         : -mkl=sequential
!configure.lib.mkl.ldflags        : -L${EBROOTIMKL}/lib/intel64 \
!                                    -lmkl_lapack95_lp64 -lmkl_intel_lp64 -lmkl_sequential -lmkl_core \
!                                    -lpthread -lm -ldl \
!                                    -Wl,-rpath -Wl,${EBROOTIMKL}/lib/intel64
!
!! if only blas is needed, for example by optimizer:
!configure.lib.blas.fflags         : -mkl=sequential
!configure.lib.blas.ldflags        : -mkl=sequential


!----------------------------------------------------------------------
! makedep
!----------------------------------------------------------------------

! Is makedepf90 installed?
! This flag is used in the 'expert.rc' settings:
my.with.makedep            :  True


!----------------------------------------------------------------------
! model data
!----------------------------------------------------------------------

! the user scratch directory:
my.scratch                    :  /scratch/shared/slr

! base path to input data files:
my.data.dir                   :  /scratch/shared/epopa/OpenLE/inputdata_Europe/v2.2
